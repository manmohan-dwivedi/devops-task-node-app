# Task Manager API

A RESTful API built using **Node.js**, **Express**, and **MongoDB** for managing tasks.  
This project follows a structured backend architecture separating routing, business logic, and database layers to maintain clarity, scalability, and maintainability.

---

## Project Structure

```
task-node-app/
â”‚
â”œâ”€â”€ .github/
â”‚   â”œâ”€â”€ workflows/
â”‚       â””â”€â”€ docker-image.yml
|
|
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.js                 # Express app & MongoDB setup
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â””â”€â”€ routes.js          # API route definitions
â”‚   â”œâ”€â”€ controllers/
â”‚   â”‚   â””â”€â”€ controllers.js     # Business logic for each route
â”‚   â””â”€â”€ models/
â”‚       â””â”€â”€ model.js           # Task schema & validation
â”œâ”€â”€ .env                       # Environment variables
â”œâ”€â”€ package.json              # Dependencies
â””â”€â”€ README.md                 # This file
```

---

# Prerequisites

- Node.js (v14 or higher)
- npm
- MongoDB Atlas account or local MongoDB installation


---
## Installation

1. **Clone/Navigate to the project:**
```bash
cd task-node-app
```

2. **Install dependencies:**
```bash
npm install
```

3. **Configure environment variables:**
Create or update the `.env` file with:
```
PORT=3000
MONGO_URI=mongodb+srv://username:password@cluster.mongodb.net/taskdb
```
Replace with your actual MongoDB connection string from MongoDB Atlas.


---
## Running the Server

Start the development server:
```bash
npm start
```

Expected output:
```
âœ… MongoDB connected
ğŸš€ Server running on port 3000
```

---
## API Endpoints

### Health Check

Base URL: `http://localhost:3000

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/health` | Check API health status |

### Task Management

Base URL: `http://localhost:3000/api/tasks`

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/` | Create a new task |
| GET | `/` | Get all tasks |
| GET | `/:id` | Get a task by ID |
| PUT | `/:id` | Update a task |
| DELETE | `/:id` | Delete a task |


---
## Testing the API

```bash
# Health Check
curl http://localhost:3000/health

# Create a task
curl -X POST http://localhost:3000/api/tasks \
  -H "Content-Type: application/json" \
  -d "{\"title\":\"Buy groceries\",\"description\":\"Milk, eggs, bread\",\"completed\":false}"

# Get all tasks
curl http://localhost:3000/api/tasks

# Get single task (replace TASK_ID with actual ID)
curl http://localhost:3000/api/tasks/TASK_ID

# Update task
curl -X PUT http://localhost:3000/api/tasks/TASK_ID \
  -H "Content-Type: application/json" \
  -d "{\"title\":\"Buy groceries\",\"completed\":true}"

# Delete task
curl -X DELETE http://localhost:3000/api/tasks/TASK_ID
```

### Using Postman

1. Import the endpoints listed in the API Endpoints table
2. Set the request body to JSON format
3. Use the example payloads shown above

## Request/Response Examples

### Create Task (POST)
**Request:**
```json
{
  "title": "Complete project",
  "description": "Finish Node.js task manager",
  "completed": false
}
```

**Response (201):**
```json
{
  "_id": "507f1f77bcf86cd799439011",
  "title": "Complete project",
  "description": "Finish Node.js task manager",
  "completed": false,
  "createdAt": "2026-02-05T10:30:00.000Z",
  "updatedAt": "2026-02-05T10:30:00.000Z"
}
```

### Get All Tasks (GET)
**Response (200):**
```json
[
  {
    "_id": "507f1f77bcf86cd799439011",
    "title": "Complete project",
    "completed": false,
    "createdAt": "2026-02-05T10:30:00.000Z"
  }
]
```

---
## Validation Rules

- **title**: Required, minimum 3 characters
- **description**: Optional, defaults to empty string
- **completed**: Optional boolean, defaults to false

---
## Error Handling

- **400**: Bad request (validation error or invalid ID)
- **404**: Task not found
- **500**: Server error
- **201**: Task created successfully
- **200**: Request successful

---
## REST Architecture & Request Flow

The API follows a layered REST architecture. Each layer has a specific responsibility to keep the system modular and easy to maintain.

### High-Level Flow

```
Client Request
â†“
Express Application (app.js)
â†“
Routes Layer
â†“
Controllers Layer
â†“
Models (Mongoose)
â†“
MongoDB Database
â†“
Response returned to client
```

### Flow of REST API under the hood

```
Routing â†’ Controller
- taskRoutes.js maps each HTTP method and path to a specific controller function in taskController.js.
- The router decides *which* controller runs based on the incoming request.

Controller â†’ Model (Database Logic)
- The controller function handles:
  - Reading request data (req.body, req.params, req.query)
  - Validating or preprocessing input (if needed)
  - Calling Mongoose model methods (Task.find(), Task.create(), Task.save(), etc.)
  - Handling errors
  - Sending the HTTP response (usually JSON)

Model â†’ MongoDB
- taskModel.js defines the schema, structure, and validation rules for a task.
- Mongoose acts as an ODM (Object Data Modeling layer), translating model operations into MongoDB queries.
- MongoDB performs data storage and retrieval.

Response Back to Client
- The controller sends the final HTTP response:
  - Success response with data (task object, list, message)
  - Or error response with appropriate HTTP status code.

Flow:

Client â†’ Express App (app.js) â†’ Router (taskRoutes.js)
â†’ Controller (taskController.js) â†’ Model (taskModel.js)
â†’ Mongoose â†’ MongoDB

Then the result flows back:

MongoDB â†’ Mongoose â†’ Controller â†’ Express Response â†’ Client (JSON)

```
---


### Cotainerization

- **Basic containerization**
    - Create Dockerfile
    
    ```bash
    FROM node:18
    
    WORKDIR /app
    
    COPY package*.json ./
    
    RUN npm install
    
    COPY . .
    
    EXPOSE 3000
    
    CMD ["npm", "start"]
    ```
    
    - Create .dockerignore
    
    ```bash
    node_modules
    npm-debug.log
    .git
    .env
    ```
    
    - Build docker image
    
    ```bash
    docker build -t devops-project1 .
    ```
    
    - Run container : use --env-file .env  for local .env file
    
    ```bash
    docker run --env-file .env -p 3000:3000 devops-project1
    ```
    
- **Professional Dockerfile (Multi-Stage + Optimization)**
    - Why Multi-Stage Build?
        
        Normal Dockerfile:
        
        ğŸ‘‰ installs everything
        ğŸ‘‰ keeps dev dependencies
        ğŸ‘‰ bigger image size
        ğŸ‘‰ slower startup
        
        Multi-stage build:
        
        ğŸ‘‰ one stage builds app
        ğŸ‘‰ final stage contains ONLY runtime requirements
        
        Think of it like:
        
        Kitchen = build stage
        
        Serving plate = runtime stage
        
    
    ```bash
    # -------- BUILD STAGE --------
    FROM node:18-alpine AS builder
    
    WORKDIR /app
    
    COPY package*.json ./
    
    RUN npm ci
    # npm ci instead of npm install Uses lockfile and Faster
    
    COPY . .
    
    # -------- RUNTIME STAGE --------
    FROM node:18-alpine
    
    WORKDIR /app
    
    # copy only necessary files from builder
    COPY --from=builder /app /app
    
    ENV NODE_ENV=production
    
    EXPOSE 3000
    
    RUN addgroup -S appgroup && adduser -S appuser -G appgroup
    USER appuser
    
    CMD ["npm", "start"]
    ```
    
    - What just happened
        
        Two containers exist during build:
        
        1ï¸âƒ£ builder stage
        
        - installs dependencies
        - prepares app
        
        2ï¸âƒ£ final runtime stage
        
        - receives only built app
        
        Benefits:
        
        - cleaner layers
        - industry-standard pattern
        - foundation for scaling later
    - Imp Security optimization - Non-root User
        
        Containers running as root = security risk.
        
        ```bash
        RUN addgroup -S appgroup && adduser -S appuser -G appgroup
        USER appuser
        ```
        
        This part uses Alpine Linux syntax (indicated by the `-S` flag) to set up the environment:
        
        - **`addgroup -S appgroup`**: Creates a **system group** (`S`) named "appgroup." Groups are used to manage permissions for multiple users at once.
        - **`&&`**: This is a logical "AND." It tells Docker to only run the next command if the first one succeeds. This keeps the image layers clean.
        - **`adduser -S appuser -G appgroup`**: Creates a **system user** (`S`) named "appuser" and immediately assigns them to the "appgroup" (`G`).

### Push container image to registry (GHCR)

- **Image name â†’ Standard naming:**

```bash
ghcr.io/<github-username>/<repo-name>:tag
```

- **Login to GHCR from terminal**
- Create token: GitHub â†’ Settings â†’ Developer Settings â†’ Personal Access Tokens
    
    ```bash
    write:packages
    read:packages
    repo
    ```
    
- Login :

```bash
echo YOUR_GITHUB_TOKEN | docker login ghcr.io -u YOUR_GITHUB_USERNAME --password-stdin
```

- Tag image ( v1, v2â€¦)

```bash
docker tag devops-project1 ghcr.io/<username>/<repo-name>:v1
```

- Push image

```bash
docker push ghcr.io/<username>/<repo-name>:v1
```

- always tag lastest version image bt **latest** tag

### CI/CD with GitHub Actions

```bash
git push
   â†“
GitHub Actions starts
   â†“
Docker image builds
   â†“
Image pushed to GHCR
   â†“
Ready for deployment
```

In repo create 

```bash
.github/
â””â”€â”€ workflows/
    â””â”€â”€ docker-image.yml
```

docker-image.yml

```bash
name: Build and Push Docker Image

on:
  push:
    branches:
      - main

jobs:
  build:
    runs-on: ubuntu-latest

    permissions:
      contents: read
      packages: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          push: true
          tags: |
            ghcr.io/${{ github.repository }}:latest
            ghcr.io/${{ github.repository }}:${{ github.sha }}
```

- what this pipeline does
    - `checkout` â†’ gets your code
    - `buildx` â†’ modern Docker builder
    - `login` â†’ secure registry auth (no PAT required)
    - `build-push` â†’ builds + pushes image
    - `tags`:
        - `latest` â†’ easy deployments
        - `commit SHA` â†’ perfect traceability
- Now push code to github and verify pipeline.
- If it fails with 403 Forbidden
    - Problem
        
        ```bash
        403 Forbidden during push to ghcr.io
        ```
        
        - GitHub Actions authenticated successfully
        - BUT does **not have permission to push package**
    - Solution
        - Pckage and repo visibilty should match.
        - Manual push earlier using PAT, GHCR may have created package with different ownership permissions.
        - Delete the package temporarily. Sometimes initial push creates permission lock.
        - Then re-run pipeline and let Actions recreate package.
        
    

### Deploy Container on Render

```bash
GitHub push
     â†“
GitHub Actions builds image
     â†“
Image pushed to GHCR
     â†“
Render pulls image
     â†“
Live running service
```

- Create Render Account, Sign up with GitHub.
- Select container image or use image URL
- Environment Variables
    - Enter variables on render
    - .env never pushed with code
- Now deploy
- Now  verify api health

```bash
name: Build and Push Docker Image

on:
  push:
    branches:
      - main

jobs:
  build:
    runs-on: ubuntu-latest

    permissions:
      contents: read
      packages: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          push: true
          tags: |
            ghcr.io/${{ github.repository }}:latest
            ghcr.io/${{ github.repository }}:${{ github.sha }}
```

- what this pipeline does
    - `checkout` â†’ gets your code
    - `buildx` â†’ modern Docker builder
    - `login` â†’ secure registry auth (no PAT required)
    - `build-push` â†’ builds + pushes image
    - `tags`:
        - `latest` â†’ easy deployments
        - `commit SHA` â†’ perfect traceability
- Now push code to github and verify pipeline.
- If it fails with 403 Forbidden
    - Problem
        
        ```bash
        403 Forbidden during push to ghcr.io
        ```
        
        - GitHub Actions authenticated successfully
        - BUT does **not have permission to push package**
    - Solution
        - Pckage and repo visibilty should match.
        - Manual push earlier using PAT, GHCR may have created package with different ownership permissions.
        - Delete the package temporarily. Sometimes initial push creates permission lock.
        - Then re-run pipeline and let Actions recreate package.
        
    

### Automatic/Continues Deployment (Deploy Hook)/ CD

```bash
git push
   â†“
CI builds image
   â†“
Image pushed to GHCR
   â†“
Render detects update
   â†“
Render pulls image
   â†“
Live service updates automatically
```

- Render does NOT automatically redeploy when new image is pushed (by default with registry images).
- **Deploy Hook**
    - Get Render Deploy Hook
        - Service â†’ Settings â†’ Deploy Hook
    - Add GitHub Secret
        - GitHub Repo â†’ Settings â†’ Secrets â†’ Actions â†’ New Repository Secret
        - Name hook and put hook URL(from render) in value
    - Update workflow for triggering deploy hook
        
        ```bash
        name: Build and Push Docker Image
        
        on:
          push:
            branches:
              - main
        
        jobs:
          build:
            runs-on: ubuntu-latest
        
            permissions:
              contents: read
              packages: write
        
            steps:
              - name: Checkout repository
                uses: actions/checkout@v4
        
              - name: Set up Docker Buildx
                uses: docker/setup-buildx-action@v3
        
              - name: Login to GitHub Container Registry
                uses: docker/login-action@v3
                with:
                  registry: ghcr.io
                  username: ${{ github.actor }}
                  password: ${{ secrets.GITHUB_TOKEN }}
        
              - name: Build and push Docker image
                uses: docker/build-push-action@v5
                with:
                  push: true
                  tags: |
                    ghcr.io/${{ github.repository }}:latest
                    ghcr.io/${{ github.repository }}:${{ github.sha }}
        
              - name: Trigger Render Deploy
                run: curl -X POST ${{ secrets.RENDER_DEPLOY_HOOK }}
        
        ```
### Cotainerization

- **Basic containerization**
    - Create Dockerfile
    
    ```bash
    FROM node:18
    
    WORKDIR /app
    
    COPY package*.json ./
    
    RUN npm install
    
    COPY . .
    
    EXPOSE 3000
    
    CMD ["npm", "start"]
    ```
    
    - Create .dockerignore
    
    ```bash
    node_modules
    npm-debug.log
    .git
    .env
    ```
    
    - Build docker image
    
    ```bash
    docker build -t devops-project1 .
    ```
    
    - Run container : use --env-file .env  for local .env file
    
    ```bash
    docker run --env-file .env -p 3000:3000 devops-project1
    ```
    
- **Professional Dockerfile (Multi-Stage + Optimization)**
    - Why Multi-Stage Build?
        
        Normal Dockerfile:
        
        ğŸ‘‰ installs everything
        ğŸ‘‰ keeps dev dependencies
        ğŸ‘‰ bigger image size
        ğŸ‘‰ slower startup
        
        Multi-stage build:
        
        ğŸ‘‰ one stage builds app
        ğŸ‘‰ final stage contains ONLY runtime requirements
        
        Think of it like:
        
        Kitchen = build stage
        
        Serving plate = runtime stage
        
    
    ```bash
    # -------- BUILD STAGE --------
    FROM node:18-alpine AS builder
    
    WORKDIR /app
    
    COPY package*.json ./
    
    RUN npm ci
    # npm ci instead of npm install Uses lockfile and Faster
    
    COPY . .
    
    # -------- RUNTIME STAGE --------
    FROM node:18-alpine
    
    WORKDIR /app
    
    # copy only necessary files from builder
    COPY --from=builder /app /app
    
    ENV NODE_ENV=production
    
    EXPOSE 3000
    
    RUN addgroup -S appgroup && adduser -S appuser -G appgroup
    USER appuser
    
    CMD ["npm", "start"]
    ```
    
    - What just happened
        
        Two containers exist during build:
        
        1ï¸âƒ£ builder stage
        
        - installs dependencies
        - prepares app
        
        2ï¸âƒ£ final runtime stage
        
        - receives only built app
        
        Benefits:
        
        - cleaner layers
        - industry-standard pattern
        - foundation for scaling later
    - Imp Security optimization - Non-root User
        
        Containers running as root = security risk.
        
        ```bash
        RUN addgroup -S appgroup && adduser -S appuser -G appgroup
        USER appuser
        ```
        
        This part uses Alpine Linux syntax (indicated by the `-S` flag) to set up the environment:
        
        - **`addgroup -S appgroup`**: Creates a **system group** (`S`) named "appgroup." Groups are used to manage permissions for multiple users at once.
        - **`&&`**: This is a logical "AND." It tells Docker to only run the next command if the first one succeeds. This keeps the image layers clean.
        - **`adduser -S appuser -G appgroup`**: Creates a **system user** (`S`) named "appuser" and immediately assigns them to the "appgroup" (`G`).

### Push container image to registry (GHCR)

- **Image name â†’ Standard naming:**

```bash
ghcr.io/<github-username>/<repo-name>:tag
```

- **Login to GHCR from terminal**
- Create token: GitHub â†’ Settings â†’ Developer Settings â†’ Personal Access Tokens
    
    ```bash
    write:packages
    read:packages
    repo
    ```
    
- Login :

```bash
echo YOUR_GITHUB_TOKEN | docker login ghcr.io -u YOUR_GITHUB_USERNAME --password-stdin
```

- Tag image ( v1, v2â€¦)

```bash
docker tag devops-project1 ghcr.io/<username>/<repo-name>:v1
```

- Push image

```bash
docker push ghcr.io/<username>/<repo-name>:v1
```

- always tag lastest version image bt **latest** tag

### CI/CD with GitHub Actions

```bash
git push
   â†“
GitHub Actions starts
   â†“
Docker image builds
   â†“
Image pushed to GHCR
   â†“
Ready for deployment
```

In repo create 

```bash
.github/
â””â”€â”€ workflows/
    â””â”€â”€ docker-image.yml
```

docker-image.yml

```bash
name: Build and Push Docker Image

on:
  push:
    branches:
      - main

jobs:
  build:
    runs-on: ubuntu-latest

    permissions:
      contents: read
      packages: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          push: true
          tags: |
            ghcr.io/${{ github.repository }}:latest
            ghcr.io/${{ github.repository }}:${{ github.sha }}
```        
- what this pipeline does
    - `checkout` â†’ gets your code
    - `buildx` â†’ modern Docker builder
    - `login` â†’ secure registry auth (no PAT required)
    - `build-push` â†’ builds + pushes image
    - `tags`:
        - `latest` â†’ easy deployments
        - `commit SHA` â†’ perfect traceability
- Now push code to github and verify pipeline.
